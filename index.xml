<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alessandro Canevaro</title>
    <link>https://alessandro-canevaro.github.io/</link>
      <atom:link href="https://alessandro-canevaro.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Alessandro Canevaro</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 08 Dec 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://alessandro-canevaro.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Alessandro Canevaro</title>
      <link>https://alessandro-canevaro.github.io/</link>
    </image>
    
    <item>
      <title>Reinforcement Learning</title>
      <link>https://alessandro-canevaro.github.io/project/vpn/</link>
      <pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://alessandro-canevaro.github.io/project/vpn/</guid>
      <description>&lt;h2 id=&#34;overall-goal-of-the-project&#34;&gt;Overall goal of the project&lt;/h2&gt;
&lt;p&gt;Value iteration networks represent a way to combine reinforcement learning with planning. The idea is fairly simple: If we know a model of the environment, it is well known that the so-called Value-iteration algorithm (See Suttons book linked above) is the optimal way to plan ahead. A ‘model’ in this context is essentially just a mapping from inputs to the reinforcement learning agent to a probability distribution, i.e. you can think about it as a softmax multi-class classifier. In reinforcement learning, we don’t know a model, but we know how to build a softmax classifier. So what that gives us is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We build a softmax classifier, which output a possible model of the environment&lt;/li&gt;
&lt;li&gt;Given this model, we can compute the optimal action by running the value-iteration algorithm&lt;/li&gt;
&lt;li&gt;The reinforcement learning agent can then output this action&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We build a softmax classifier, which output a possible model of the environment
Given this model, we can compute the optimal action by running the value-iteration algorithm
The reinforcement learning agent can then output this action
The idea is pretty simple: When we interact with the environment, we get information about how good or bad an action is. We can use this information to train the neural network using gradient descent. The neat thing is that the combined algorithm ends up resembling a standard convolution neural network. Since we are integrating planning with the reinforcement learning algorithm, this makes the combined model able to generalize to new situations. You can read more in the original paper: &lt;a href=&#34;https://arxiv.org/abs/1602.02867&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1602.02867&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Systems Optimization</title>
      <link>https://alessandro-canevaro.github.io/project/optim/</link>
      <pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://alessandro-canevaro.github.io/project/optim/</guid>
      <description>&lt;h2 id=&#34;activity-goal&#34;&gt;Activity goal&lt;/h2&gt;
&lt;p&gt;As an input you are given:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1) An application model consisting of a set of TT tasks and a set of ET tasks,&lt;/li&gt;
&lt;li&gt;(2) An architecture model consisting of one core that schedules TT tasks with timeline scheduling and ET tasks with polling servers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You will have to design and implement an optimization algorithm that determines an optimized solution which consists of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1) The number of polling servers, which then become extra TT tasks.&lt;/li&gt;
&lt;li&gt;(2) For each polling server (task), the period, budget, and deadline.&lt;/li&gt;
&lt;li&gt;(3) Which sub-sets of ET tasks are handled within the respective polling servers.&lt;/li&gt;
&lt;li&gt;(4) A TT schedule such that also the TT tasks are schedulable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The solution should be optimized such that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1) Both the TT and ET tasks are schedulable, i.e., they complete before their deadlines.&lt;/li&gt;
&lt;li&gt;(2) The ET task separation constraints are satisfied.&lt;/li&gt;
&lt;li&gt;(3) The average worst-case response times (WCRT) of all tasks (TT and ET) is minimized.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Object Tracking and Classification</title>
      <link>https://alessandro-canevaro.github.io/project/perception/</link>
      <pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate>
      <guid>https://alessandro-canevaro.github.io/project/perception/</guid>
      <description>&lt;h2 id=&#34;activity-goal&#34;&gt;Activity goal&lt;/h2&gt;
&lt;p&gt;The objective of the final project is to detect, track, predict, and classify moving objects, whether
they are occluded or unoccluded. In our scenario, we are given a stereo camera setup looking over
a conveyor belt. Random objects are placed at one end of the conveyor belt at regular intervals.
This simulates a general-purpose case where these objects need to have their attributes recognized
in real-time. The main steps required to achieve our project objectives are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Camera Calibration and Rectification&lt;/li&gt;
&lt;li&gt;Object Detection and Tracking&lt;/li&gt;
&lt;li&gt;Object Classification&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning Operation</title>
      <link>https://alessandro-canevaro.github.io/project/mlops/</link>
      <pubDate>Fri, 21 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://alessandro-canevaro.github.io/project/mlops/</guid>
      <description>&lt;h2 id=&#34;overall-goal-of-the-project&#34;&gt;Overall goal of the project&lt;/h2&gt;
&lt;p&gt;The main goal of this project is to apply what we have learned in this course about MLOps to a simple machine/deep learning problem. We aim to make our whole pipeline as understandable and efficient as possible, using the tools we have been given - having good and clear structure and setup, adding comments to the code to make it easier to understand and so on. We will tackle an image classification problem and try to get as good results as possible, given our timeframe.&lt;/p&gt;
&lt;h2 id=&#34;what-framework-are-you-going-to-use&#34;&gt;What framework are you going to use&lt;/h2&gt;
&lt;p&gt;It was decided to use the &lt;a href=&#34;https://github.com/kornia/kornia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kornia&lt;/a&gt; framework as it was deemed the best suited for this project which involves image classification.&lt;/p&gt;
&lt;h2 id=&#34;how-to-you-intend-to-include-the-framework-into-your-project&#34;&gt;How to you intend to include the framework into your project&lt;/h2&gt;
&lt;p&gt;The main purpose of the Kornia framework in this project is &lt;a href=&#34;https://kornia.readthedocs.io/en/latest/applications/image_augmentations.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;data augmentation&lt;/a&gt;. The goal is to obtain a larger dataset, adding transformed images to the dataset we have already. Specifically, the focus is on the augmentation, the color, and the enhance modules of Kornia as these will provide all the necessary functions to manipulate the images (i.e. rotating, scaling, normalizing, etc.).&lt;/p&gt;
&lt;h2 id=&#34;what-data-are-you-going-to-run-on&#34;&gt;What data are you going to run on&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Kaggle link:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/grassknoted/asl-alphabet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.kaggle.com/grassknoted/asl-alphabet&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;General description:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The data set is a collection of images of hands signing the American Sign Language alphabet, separated in 29 folders which represent the various classes.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Size:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Train data: 87,000 images, 200x200 pixels each.&lt;/li&gt;
&lt;li&gt;Test data: 29 images.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Classes:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;26 for the letters A-Z.&lt;/li&gt;
&lt;li&gt;3 classes for SPACE, DELETE and NOTHING.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-deep-learning-models-do-you-expect-to-use&#34;&gt;What deep learning models do you expect to use&lt;/h2&gt;
&lt;p&gt;We will use mostly CNN (convolutional neural networks). Our focus will be on the overall architecture and parameters of our network (number of layers, padding, strides, max-pooling etc.).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Disease Tracker</title>
      <link>https://alessandro-canevaro.github.io/project/mbse/</link>
      <pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://alessandro-canevaro.github.io/project/mbse/</guid>
      <description>&lt;h2 id=&#34;activity-goal&#34;&gt;Activity goal&lt;/h2&gt;
&lt;p&gt;The focus of this project is to identify if a wearable device can be used to reduce the spreading rate of COVID-19 by tracking a person’s body temperature and oxygen saturation in the blood in order to determine with a certain probability whether or not she might be infected, and if that is the case, warn her as well as the others in close proximity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robot Operating System (ROS)</title>
      <link>https://alessandro-canevaro.github.io/project/ros/</link>
      <pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://alessandro-canevaro.github.io/project/ros/</guid>
      <description>&lt;h2 id=&#34;activity-goal&#34;&gt;Activity goal&lt;/h2&gt;
&lt;p&gt;This project focuses on the development of software for a simulated robotic system, Turtlebot 3. This system should be capable of navigating a mostly predefined operational environment and to identify targets using information coming from QR codes. The environment is mostly predefined however the obstacles and targets, QR codes, can be spawned in pseudo random locations.&lt;/p&gt;
&lt;p&gt;There will always be five QR codes, which spawn in the predefined operational area, however, the specific location of these QR codes will not be the same all the time. All QR codes contain four attributes. The attributes are the coordinates of the current QR target, the coordinates of the next QR target, and the ID of the QR code, which can be anywhere from 1 to 5. Lastly, a letter is also an attribute and these must be structured in the correct order in order to extract a message. Furthermore, the last QR code, i.e. the QR code number
5, has, as the next qr code coordinates, the coordinates of the first QR code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Image Detection</title>
      <link>https://alessandro-canevaro.github.io/project/boats/</link>
      <pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://alessandro-canevaro.github.io/project/boats/</guid>
      <description>&lt;h2 id=&#34;activity-goal&#34;&gt;Activity goal&lt;/h2&gt;
&lt;p&gt;The aim is to develop a computer vision algorithm that can detect boats in an image. Specifically,
it should draw a rectangular bounding box around each founded boat, and it should work correctly
even when the image has no boats in it. Note that boats can appear from any perspective, and they
can be very different: from small dinghies to cruise ships. The intersection over union (IoU) metric
is used on the provided test images to measure the algorithm’s performance.&lt;/p&gt;
&lt;h2 id=&#34;adopted-approach&#34;&gt;Adopted approach&lt;/h2&gt;
&lt;p&gt;Due to the great variety of boats and the availability of a large data set of images, a machine learning
approach seems promising. Hence the use of a neural network in the proposed solution. In particular,
the developed algorithm initially extracts features (key points and descriptors) from the image, then
classifies the descriptors as boat/non-boat, and lastly, builds the bounding boxes. The last step is,
in turn, composed of several sub-steps. The algorithm first generates a probability density function
(PDF) of the potential locations of the boats. The PDF modes are then clustered using the mean
shift algorithm, and finally, the bounding boxes are generated with a simple iterative procedure. The
complete algorithm is here summarized:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Extract features key points and descriptors from the to-be-detected image.&lt;/li&gt;
&lt;li&gt;Classify each descriptor as boat/non-boat using the trained classifier.&lt;/li&gt;
&lt;li&gt;Generate the probability map of the possible positions of the boats.&lt;/li&gt;
&lt;li&gt;Find and cluster the modes of the above probability density using the mean shift algorithm.&lt;/li&gt;
&lt;li&gt;Prune and suppress &amp;ldquo;weak&amp;rdquo; clusters.&lt;/li&gt;
&lt;li&gt;For each cluster, generate the bounding box.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Control of the SCARA robot</title>
      <link>https://alessandro-canevaro.github.io/project/scara/</link>
      <pubDate>Sun, 13 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://alessandro-canevaro.github.io/project/scara/</guid>
      <description>&lt;h2 id=&#34;activity-goal&#34;&gt;Activity goal&lt;/h2&gt;
&lt;p&gt;After having derived the equations of motion for the SCARA robot, 2 constrol solutions were designed, implemented and tested in MATLAB and simulink for trajectory tracking purposes. Specifically the first is a PD with gravity compensation controller, while the second one is a Feedback linearization controller.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Matematica Dolce</title>
      <link>https://alessandro-canevaro.github.io/project/dolce/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://alessandro-canevaro.github.io/project/dolce/</guid>
      <description>&lt;h2 id=&#34;activity-goal&#34;&gt;Activity goal&lt;/h2&gt;
&lt;p&gt;Matematica Dolce is a handbook of mathematics for secondary school written by several hands and with an open-source licence.
The collaborative mode allows a strong sharing in the choice and construction of materials and ways of presenting the topics.
Then book is freely downloadable from &lt;a href=&#34;https://www.matematicadolce.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.matematicadolce.eu/&lt;/a&gt;, and it is also available as LAMBDA book for blind students at &lt;a href=&#34;https://ddmath.eu/en/matematica-dolce/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ddmath.eu/en/matematica-dolce/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I collaborated with the project coordinator (prof. D. Zambelli) in writing a program to automatically generate maths exercises.
The code is written in Python and the output is automatically formatted in LaTeX.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://alessandro-canevaro.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://alessandro-canevaro.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
